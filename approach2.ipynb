{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy transformers numpy scikit-learn certifi numpy"
      ],
      "metadata": {
        "id": "yi0hw2nVzvWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWpXt9L1yHkP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import sqlite3\n",
        "import time\n",
        "from datetime import datetime\n",
        "import requests\n",
        "import tweepy\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class CompanyAgent:\n",
        "    def __init__(self, company_name, description, twitter_handle,\n",
        "                 twitter_api_key=None, twitter_api_secret=None,\n",
        "                 twitter_access_token=None, twitter_access_token_secret=None,\n",
        "                 mediastack_api_key=None, serpapi_api_key=None):\n",
        "        self.company_name = company_name\n",
        "        self.description = description\n",
        "        self.twitter_handle = twitter_handle\n",
        "\n",
        "        # API Keys\n",
        "        self.mediastack_api_key = mediastack_api_key or os.environ.get(\"MEDIASTACK_API_KEY\")\n",
        "        self.serpapi_api_key = serpapi_api_key or os.environ.get(\"SERPAPI_API_KEY\")\n",
        "        self.twitter_api_key = twitter_api_key or os.environ.get(\"TWITTER_API_KEY\")\n",
        "        self.twitter_api_secret = twitter_api_secret or os.environ.get(\"TWITTER_API_SECRET\")\n",
        "        self.twitter_access_token = twitter_access_token or os.environ.get(\"TWITTER_ACCESS_TOKEN\")\n",
        "        self.twitter_access_token_secret = twitter_access_token_secret or os.environ.get(\"TWITTER_ACCESS_TOKEN_SECRET\")\n",
        "\n",
        "        # Each company gets its own database (isolated knowledge)\n",
        "        self.memory_db = f\"memory_{self.company_name.replace(' ', '_').lower()}.db\"\n",
        "        self.initialize_db()\n",
        "        self.texts = []  # Texts to build the vector store\n",
        "\n",
        "        # Initialize sentiment analysis pipeline\n",
        "        try:\n",
        "            self.sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "            print(f\"[{self.company_name}] Sentiment analyzer initialized successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.company_name}] Error initializing sentiment analyzer: {e}\")\n",
        "            self.sentiment_analyzer = None\n",
        "\n",
        "        # Initialize Twitter API\n",
        "        self.initialize_twitter_api()\n",
        "\n",
        "        # Initialize text generation model\n",
        "        try:\n",
        "            self.text_generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "            print(f\"[{self.company_name}] Text generation model initialized successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.company_name}] Error initializing text generation model: {e}\")\n",
        "            self.text_generator = None\n",
        "\n",
        "        # Initialize embedding model (using distilbert for a lightweight approach)\n",
        "        try:\n",
        "            self.embedding_model = pipeline('feature-extraction', model='distilbert-base-uncased')\n",
        "            print(f\"[{self.company_name}] Embedding model initialized successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.company_name}] Error initializing embedding model: {e}\")\n",
        "            self.embedding_model = None\n",
        "\n",
        "        # Load previously stored texts from the company's database\n",
        "        self.load_texts_from_db()\n",
        "\n",
        "        # Initialize vector store if texts are available\n",
        "        if self.texts:\n",
        "            self.initialize_vector_store()\n",
        "\n",
        "    def initialize_db(self):\n",
        "        \"\"\"Sets up a separate SQLite database for each company.\"\"\"\n",
        "        try:\n",
        "            self.conn = sqlite3.connect(self.memory_db)\n",
        "            self.cursor = self.conn.cursor()\n",
        "\n",
        "            # Table for general company data\n",
        "            self.cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS company_data (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    data TEXT,\n",
        "                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            # Table to track processed tweets\n",
        "            self.cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS processed_tweets (\n",
        "                    tweet_id TEXT PRIMARY KEY,\n",
        "                    processed_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            # Table for stored texts\n",
        "            self.cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS stored_texts (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    text TEXT UNIQUE,\n",
        "                    embedding BLOB,\n",
        "                    added_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            # Table for news items\n",
        "            self.cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS news_items (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    title TEXT,\n",
        "                    description TEXT,\n",
        "                    source TEXT,\n",
        "                    url TEXT UNIQUE,\n",
        "                    published_at TEXT,\n",
        "                    collected_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            # Table for social engagements\n",
        "            self.cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS social_engagements (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    tweet_id TEXT UNIQUE,\n",
        "                    action TEXT,\n",
        "                    content TEXT,\n",
        "                    performed_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            self.conn.commit()\n",
        "            print(f\"[{self.company_name}] Database initialized successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.company_name}] Error initializing database: {e}\")\n",
        "            raise\n",
        "\n",
        "    def load_texts_from_db(self):\n",
        "        \"\"\"Load previously stored texts from the database.\"\"\"\n",
        "        try:\n",
        "            self.cursor.execute(\"SELECT text, embedding FROM stored_texts\")\n",
        "            rows = self.cursor.fetchall()\n",
        "            if rows:\n",
        "                self.texts = [row[0] for row in rows]\n",
        "                self.embeddings = []\n",
        "                for row in rows:\n",
        "                    if row[1] is not None:\n",
        "                        self.embeddings.append(np.frombuffer(row[1], dtype=np.float32))\n",
        "                    else:\n",
        "                        # If embedding is missing, compute it now\n",
        "                        if self.embedding_model:\n",
        "                            embedding = self.get_embedding(row[0])\n",
        "                            if embedding is not None:\n",
        "                                self.embeddings.append(embedding)\n",
        "                                embedding_binary = embedding.tobytes()\n",
        "                                self.cursor.execute(\"UPDATE stored_texts SET embedding = ? WHERE text = ?\",\n",
        "                                                    (embedding_binary, row[0]))\n",
        "                self.conn.commit()\n",
        "                print(f\"[{self.company_name}] Loaded {len(self.texts)} texts from database.\")\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.company_name}] Error loading texts from database: {e}\")\n",
        "            self.texts = []\n",
        "            self.embeddings = []\n",
        "\n",
        "    def get_embedding(self, text):\n",
        "        \"\"\"Generate embedding for a text using the embedding model.\"\"\"\n",
        "        if not self.embedding_model:\n",
        "            return None\n",
        "        try:\n",
        "            features = self.embedding_model(text, truncation=True, max_length=512)\n",
        "            # Use the mean of token embeddings as the sentence embedding\n",
        "            embedding = np.mean(features[0], axis=0)\n",
        "            return embedding.astype(np.float32)\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.company_name}] Error generating embedding: {e}\")\n",
        "            return None\n",
        "\n",
        "    def save_texts_to_db(self, new_texts):\n",
        "        \"\"\"Save new texts and their embeddings to the database.\"\"\"\n",
        "        try:\n",
        "            for text in new_texts:\n",
        "                embedding = self.get_embedding(text)\n",
        "                if embedding is not None:\n",
        "                    embedding_binary = embedding.tobytes()\n",
        "                    self.cursor.execute(\n",
        "                        \"INSERT OR IGNORE INTO stored_texts (text, embedding) VALUES (?, ?)\",\n",
        "                        (text, embedding_binary)\n",
        "                    )\n",
        "                else:\n",
        "                    self.cursor.execute(\n",
        "                        \"INSERT OR IGNORE INTO stored_texts (text) VALUES (?)\",\n",
        "                        (text,)\n",
        "                    )\n",
        "            self.conn.commit()\n",
        "            print(f\"[{self.company_name}] Saved {len(new_texts)} new texts to database.\")\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.company_name}] Error saving texts to database: {e}\")\n",
        "\n",
        "    def initialize_vector_store(self):\n",
        "        \"\"\"Initialize a simple vector store from stored embeddings.\"\"\"\n",
        "        if not self.texts or not hasattr(self, 'embeddings') or not self.embeddings:\n",
        "            print(f\"[{self.company_name}] No texts or embeddings available to create vector store.\")\n",
        "            return\n",
        "        try:\n",
        "            print(f\"[{self.company_name}] Vector store initialized with {len(self.texts)} texts.\")\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.company_name}] Error initializing vector store: {e}\")\n",
        "\n",
        "    def initialize_twitter_api(self):\n",
        "        \"\"\"Initialize the Twitter API connection using Tweepy.\"\"\"\n",
        "        if not all([self.twitter_api_key, self.twitter_api_secret,\n",
        "                    self.twitter_access_token, self.twitter_access_token_secret]):\n",
        "            print(f\"[{self.company_name}] Twitter API credentials not provided or incomplete.\")\n",
        "            self.twitter_api = None\n",
        "            return\n",
        "        try:\n",
        "            auth = tweepy.OAuthHandler(self.twitter_api_key, self.twitter_api_secret)\n",
        "            auth.set_access_token(self.twitter_access_token, self.twitter_access_token_secret)\n",
        "            self.twitter_api = tweepy.API(auth)\n",
        "            self.twitter_api.verify_credentials()\n",
        "            print(f\"[{self.company_name}] Twitter API initialized successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.company_name}] Error initializing Twitter API: {e}\")\n",
        "            self.twitter_api = None\n",
        "\n",
        "    def add_texts_to_vector_store(self, texts):\n",
        "        \"\"\"Adds new texts to the vector store.\"\"\"\n",
        "        if not texts:\n",
        "            return\n",
        "        new_embeddings = []\n",
        "        for text in texts:\n",
        "            embedding = self.get_embedding(text)\n",
        "            if embedding is not None:\n",
        "                new_embeddings.append(embedding)\n",
        "        if len(new_embeddings) == len(texts):\n",
        "            self.save_texts_to_db(texts)\n",
        "            self.texts.extend(texts)\n",
        "            if hasattr(self, 'embeddings'):\n",
        "                self.embeddings.extend(new_embeddings)\n",
        "            else:\n",
        "                self.embeddings = new_embeddings\n",
        "            print(f\"[{self.company_name}] Added {len(texts)} texts to vector store.\")\n",
        "        else:\n",
        "            print(f\"[{self.company_name}] Only {len(new_embeddings)} embeddings generated for {len(texts)} texts.\")\n",
        "            self.save_texts_to_db(texts)\n",
        "\n",
        "    def similarity_search(self, query_text, k=3):\n",
        "        \"\"\"Search for similar texts in the vector store.\"\"\"\n",
        "        if not hasattr(self, 'embeddings') or not self.embeddings or not self.texts:\n",
        "            return []\n",
        "        query_embedding = self.get_embedding(query_text)\n",
        "        if query_embedding is None:\n",
        "            return []\n",
        "        similarities = []\n",
        "        for i, embedding in enumerate(self.embeddings):\n",
        "            query_reshaped = query_embedding.reshape(1, -1)\n",
        "            embedding_reshaped = embedding.reshape(1, -1)\n",
        "            sim = cosine_similarity(query_reshaped, embedding_reshaped)[0][0]\n",
        "            similarities.append((i, sim))\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "        results = []\n",
        "        for i, sim in similarities[:k]:\n",
        "            results.append({\n",
        "                \"text\": self.texts[i],\n",
        "                \"similarity\": sim\n",
        "            })\n",
        "        return results\n",
        "\n",
        "    def is_tweet_processed(self, tweet_id):\n",
        "        \"\"\"Check if a tweet has already been processed.\"\"\"\n",
        "        self.cursor.execute(\"SELECT tweet_id FROM processed_tweets WHERE tweet_id = ?\", (tweet_id,))\n",
        "        return self.cursor.fetchone() is not None\n",
        "\n",
        "    def mark_tweet_processed(self, tweet_id):\n",
        "        \"\"\"Mark a tweet as processed.\"\"\"\n",
        "        self.cursor.execute(\"INSERT OR IGNORE INTO processed_tweets (tweet_id) VALUES (?)\", (tweet_id,))\n",
        "        self.conn.commit()\n",
        "\n",
        "    def collect_social_media_data(self):\n",
        "        \"\"\"Collect tweets using SerpApi.\"\"\"\n",
        "        print(f\"[{self.company_name}] Collecting social media data using SerpApi...\")\n",
        "        if not self.serpapi_api_key:\n",
        "            print(f\"[{self.company_name}] SerpApi API key not provided.\")\n",
        "            return []\n",
        "        # Build query as \"<twitter_handle> twitter\" with a fixed location (adjust as needed)\n",
        "        params = {\n",
        "            \"q\": f\"{self.twitter_handle} twitter\",\n",
        "            \"location\": \"Austin,Texas,United+States\",\n",
        "            \"hl\": \"en\",\n",
        "            \"gl\": \"us\",\n",
        "            \"api_key\": self.serpapi_api_key\n",
        "        }\n",
        "        url = \"https://serpapi.com/search.json\"\n",
        "        tweets = []\n",
        "        try:\n",
        "            response = requests.get(url, params=params, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            if \"twitter_results\" in data:\n",
        "                twitter_results = data[\"twitter_results\"]\n",
        "                if \"tweets\" in twitter_results:\n",
        "                    for tweet in twitter_results[\"tweets\"]:\n",
        "                        tweet_url = tweet.get(\"link\", \"\")\n",
        "                        tweet_id = \"\"\n",
        "                        if \"status/\" in tweet_url:\n",
        "                            tweet_id = tweet_url.split(\"status/\")[-1].split(\"?\")[0]\n",
        "                        tweets.append({\n",
        "                            \"tweet_id\": tweet_id,\n",
        "                            \"post\": tweet.get(\"snippet\", \"\"),\n",
        "                            \"created_at\": tweet.get(\"published_date\", \"\"),\n",
        "                            \"url\": tweet_url\n",
        "                        })\n",
        "                    print(f\"[{self.company_name}] Collected {len(tweets)} tweets.\")\n",
        "                else:\n",
        "                    print(f\"[{self.company_name}] No tweets found in the twitter_results.\")\n",
        "            else:\n",
        "                print(f\"[{self.company_name}] No twitter_results found in API response.\")\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.company_name}] Error fetching tweets using SerpApi: {e}\")\n",
        "        return tweets\n",
        "\n",
        "    def collect_news_data(self):\n",
        "        \"\"\"Collect news data using MediaStack API.\"\"\"\n",
        "        print(f\"[{self.company_name}] Collecting news using MediaStack API...\")\n",
        "        if not self.mediastack_api_key:\n",
        "            print(f\"[{self.company_name}] MediaStack API key not provided.\")\n",
        "            return []\n",
        "        url = \"http://api.mediastack.com/v1/news\"\n",
        "        params = {\n",
        "            'access_key': self.mediastack_api_key,\n",
        "            'keywords': self.company_name,\n",
        "            'languages': 'en',\n",
        "            'limit': 10,\n",
        "            'sort': 'published_desc'\n",
        "        }\n",
        "        news_data = []\n",
        "        try:\n",
        "            response = requests.get(url, params=params, timeout=15)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                if 'data' in data and data['data']:\n",
        "                    for item in data['data']:\n",
        "                        if not item.get('title') or not item.get('description'):\n",
        "                            continue\n",
        "                        news_item = {\n",
        "                            \"title\": item.get('title', ''),\n",
        "                            \"description\": item.get('description', ''),\n",
        "                            \"source\": item.get('source', ''),\n",
        "                            \"url\": item.get('url', ''),\n",
        "                            \"published_at\": item.get('published_at', ''),\n",
        "                            \"collected_at\": datetime.now().isoformat()\n",
        "                        }\n",
        "                        news_data.append(news_item)\n",
        "                        self.cursor.execute(\"\"\"\n",
        "                            INSERT OR IGNORE INTO news_items\n",
        "                            (title, description, source, url, published_at, collected_at)\n",
        "                            VALUES (?, ?, ?, ?, ?, ?)\n",
        "                        \"\"\", (\n",
        "                            news_item[\"title\"],\n",
        "                            news_item[\"description\"],\n",
        "                            news_item[\"source\"],\n",
        "                            news_item[\"url\"],\n",
        "                            news_item[\"published_at\"],\n",
        "                            news_item[\"collected_at\"]\n",
        "                        ))\n",
        "                    self.conn.commit()\n",
        "                    print(f\"[{self.company_name}] Collected {len(news_data)} news items.\")\n",
        "                else:\n",
        "                    print(f\"[{self.company_name}] No news data found in the API response.\")\n",
        "            else:\n",
        "                print(f\"[{self.company_name}] MediaStack API returned status code {response.status_code}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.company_name}] Error fetching news: {e}\")\n",
        "        return news_data\n",
        "\n",
        "    def analyze_sentiment(self, text):\n",
        "        \"\"\"Runs sentiment analysis on the given text using DistilBERT.\"\"\"\n",
        "        if not self.sentiment_analyzer:\n",
        "            return {\"label\": \"NEUTRAL\", \"score\": 0.5}\n",
        "        try:\n",
        "            result = self.sentiment_analyzer(text)\n",
        "            return result[0]\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.company_name}] Error in sentiment analysis: {e}\")\n",
        "            return {\"label\": \"NEUTRAL\", \"score\": 0.5}\n",
        "\n",
        "    def generate_engagement_response(self, tweet_text):\n",
        "        \"\"\"\n",
        "        Generates a context-aware comment using a RAG-like approach:\n",
        "        Retrieves similar context and uses GPT-2 for generating a friendly comment.\n",
        "        \"\"\"\n",
        "        if not self.text_generator:\n",
        "            import random\n",
        "            templates = [\n",
        "                f\"Thanks for sharing! {self.company_name} values your feedback.\",\n",
        "                f\"Interesting point! We at {self.company_name} are always listening.\",\n",
        "                f\"Thank you for engaging with {self.company_name}!\"\n",
        "            ]\n",
        "            return random.choice(templates)\n",
        "        try:\n",
        "            similar_docs = self.similarity_search(tweet_text, k=3)\n",
        "            context_text = \" \".join([doc[\"text\"] for doc in similar_docs])[:300]\n",
        "            prompt = f\"\"\"\n",
        "Tweet: {tweet_text}\n",
        "\n",
        "Company: {self.company_name}\n",
        "Company Description: {self.description}\n",
        "\n",
        "Relevant context: {context_text}\n",
        "\n",
        "Generate a friendly and professional comment (around 30 words) as {self.company_name}'s response:\n",
        "\"\"\"\n",
        "            max_length = min(len(prompt.split()) + 50, 512)\n",
        "            generation = self.text_generator(prompt, max_new_tokens=max_length, num_return_sequences=1)[0]['generated_text']\n",
        "            # Attempt to extract the generated response\n",
        "            response_section = generation.split(f\"{self.company_name}'s response\")\n",
        "            if len(response_section) > 1:\n",
        "                response_only = response_section[-1].strip()\n",
        "                if response_only.startswith(\":\"):\n",
        "                    response_only = response_only[1:].strip()\n",
        "            else:\n",
        "                sentences = generation.split('.')[-3:]\n",
        "                response_only = '.'.join(sentences).strip()\n",
        "            if len(response_only.split()) > 40:\n",
        "                response_only = \" \".join(response_only.split()[:40]) + \"...\"\n",
        "            if self.company_name not in response_only:\n",
        "                response_only += f\" - {self.company_name}\"\n",
        "            print(f\"[{self.company_name}] Generated response: {response_only}\")\n",
        "            return response_only\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.company_name}] Error generating engagement response: {e}\")\n",
        "            return f\"Thank you for your message. {self.company_name} appreciates your feedback.\"\n",
        "\n",
        "    def has_engaged_with_tweet(self, tweet_id, action=None):\n",
        "        \"\"\"Check if we've already engaged with this tweet.\"\"\"\n",
        "        query = \"SELECT action FROM social_engagements WHERE tweet_id = ?\"\n",
        "        params = (tweet_id,)\n",
        "        if action:\n",
        "            query += \" AND action = ?\"\n",
        "            params = (tweet_id, action)\n",
        "        self.cursor.execute(query, params)\n",
        "        return self.cursor.fetchone() is not None\n",
        "\n",
        "    def record_engagement(self, tweet_id, action, content=None):\n",
        "        \"\"\"Record that we've engaged with a tweet.\"\"\"\n",
        "        self.cursor.execute(\n",
        "            \"INSERT OR IGNORE INTO social_engagements (tweet_id, action, content) VALUES (?, ?, ?)\",\n",
        "            (tweet_id, action, content)\n",
        "        )\n",
        "        self.conn.commit()\n",
        "\n",
        "    def engage_social_media(self, social_data, news_data):\n",
        "        \"\"\"Processes tweets and news data, then takes actions accordingly.\"\"\"\n",
        "        texts_to_add = []\n",
        "        for post in social_data:\n",
        "            tweet_id = post[\"tweet_id\"]\n",
        "            if self.is_tweet_processed(tweet_id):\n",
        "                continue\n",
        "            texts_to_add.append(post['post'])\n",
        "            sentiment = self.analyze_sentiment(post[\"post\"])\n",
        "            action = \"Monitor\"\n",
        "            generated_response = None\n",
        "            if not self.has_engaged_with_tweet(tweet_id):\n",
        "                if sentiment[\"label\"] == \"POSITIVE\" or sentiment[\"score\"] > 0.6:\n",
        "                    generated_response = self.generate_engagement_response(post[\"post\"])\n",
        "                    action = \"Comment\"\n",
        "                    self.record_engagement(tweet_id, action, generated_response)\n",
        "                elif sentiment[\"score\"] > 0.3:\n",
        "                    action = \"Like\"\n",
        "                    self.record_engagement(tweet_id, action)\n",
        "            print(f\"[{self.company_name}] Processed Tweet (ID: {tweet_id[:6]}..., URL: {post['url']}): Action: {action}\")\n",
        "            self.mark_tweet_processed(tweet_id)\n",
        "            data_json = json.dumps({\n",
        "                \"source\": \"twitter\",\n",
        "                \"tweet_id\": tweet_id,\n",
        "                \"url\": post[\"url\"],\n",
        "                \"post\": post[\"post\"],\n",
        "                \"sentiment\": sentiment,\n",
        "                \"action_taken\": action,\n",
        "                \"response_content\": generated_response,\n",
        "                \"processed_at\": datetime.now().isoformat()\n",
        "            })\n",
        "            self.cursor.execute(\"INSERT INTO company_data (data) VALUES (?)\", (data_json,))\n",
        "            self.conn.commit()\n",
        "        for news in news_data:\n",
        "            texts_to_add.append(news[\"title\"])\n",
        "            if news.get(\"description\"):\n",
        "                texts_to_add.append(news[\"description\"])\n",
        "            data_json = json.dumps({\n",
        "                \"source\": \"news\",\n",
        "                \"title\": news[\"title\"],\n",
        "                \"description\": news.get(\"description\", \"\"),\n",
        "                \"source_name\": news.get(\"source\", \"\"),\n",
        "                \"url\": news.get(\"url\", \"\"),\n",
        "                \"collected_at\": news.get(\"collected_at\", datetime.now().isoformat())\n",
        "            })\n",
        "            self.cursor.execute(\"INSERT INTO company_data (data) VALUES (?)\", (data_json,))\n",
        "            self.conn.commit()\n",
        "            print(f\"[{self.company_name}] Processed news item: '{news['title']}', URL: {news.get('url', '')}\")\n",
        "        if texts_to_add:\n",
        "            self.add_texts_to_vector_store(texts_to_add)\n",
        "\n",
        "    def generate_scheduled_post(self):\n",
        "        \"\"\"Generate a post based on recent news and company knowledge.\"\"\"\n",
        "        if not self.text_generator:\n",
        "            return None\n",
        "        try:\n",
        "            self.cursor.execute(\"\"\"\n",
        "                SELECT title, description FROM news_items\n",
        "                ORDER BY collected_at DESC LIMIT 5\n",
        "            \"\"\")\n",
        "            recent_news = self.cursor.fetchall()\n",
        "            news_context = \"\"\n",
        "            for title, description in recent_news:\n",
        "                if title and description:\n",
        "                    news_context += f\"- {title}: {description[:100]}...\\n\"\n",
        "            prompt = f\"\"\"\n",
        "Company: {self.company_name}\n",
        "Description: {self.description}\n",
        "\n",
        "Recent news:\n",
        "{news_context}\n",
        "\n",
        "Generate a short, engaging social media post (maximum 280 characters) for {self.company_name} based on recent developments:\n",
        "\"\"\"\n",
        "            max_length = len(prompt.split()) + 50\n",
        "            generation = self.text_generator(prompt, max_new_tokens=max_length, num_return_sequences=1)[0]['generated_text']\n",
        "            if \"social media post\" in generation:\n",
        "                post_content = generation.split(\"social media post\")[-1].strip()\n",
        "                if post_content.startswith(\":\"):\n",
        "                    post_content = post_content[1:].strip()\n",
        "            else:\n",
        "                sentences = generation.split('.')[-3:]\n",
        "                post_content = '.'.join(sentences).strip()\n",
        "            if len(post_content) > 280:\n",
        "                post_content = post_content[:277] + \"...\"\n",
        "            return post_content\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.company_name}] Error generating scheduled post: {e}\")\n",
        "            return None\n",
        "\n",
        "    def create_scheduled_posts(self, num_posts=1):\n",
        "        \"\"\"Create scheduled posts based on company knowledge.\"\"\"\n",
        "        created_posts = []\n",
        "        for _ in range(num_posts):\n",
        "            post_content = self.generate_scheduled_post()\n",
        "            if post_content:\n",
        "                timestamp = datetime.now().isoformat()\n",
        "                post_data = {\n",
        "                    \"type\": \"scheduled_post\",\n",
        "                    \"content\": post_content,\n",
        "                    \"created_at\": timestamp\n",
        "                }\n",
        "                data_json = json.dumps(post_data)\n",
        "                self.cursor.execute(\"INSERT INTO company_data (data) VALUES (?)\", (data_json,))\n",
        "                self.conn.commit()\n",
        "                created_posts.append(post_content)\n",
        "                print(f\"[{self.company_name}] Created scheduled post: {post_content[:50]}...\")\n",
        "        return created_posts\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Runs the agent: collects social media and news data, then engages.\"\"\"\n",
        "        try:\n",
        "            print(f\"\\n[{self.company_name}] Running agent at {datetime.now().isoformat()}\")\n",
        "            social_data = self.collect_social_media_data()\n",
        "            news_data = self.collect_news_data()\n",
        "            self.engage_social_media(social_data, news_data)\n",
        "            self.create_scheduled_posts(1)\n",
        "            print(f\"[{self.company_name}] Run completed successfully.\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.company_name}] Error during run: {e}\")\n",
        "\n",
        "    def _del_(self):\n",
        "        \"\"\"Cleanup: close the database connection.\"\"\"\n",
        "        try:\n",
        "            if hasattr(self, 'conn') and self.conn:\n",
        "                self.conn.close()\n",
        "                print(f\"[{self.company_name}] Database connection closed.\")\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.company_name}] Error closing database connection: {e}\")\n",
        "\n",
        "def trigger_training(agents):\n",
        "    \"\"\"Triggers data collection and engagement for all agents.\"\"\"\n",
        "    print(f\"\\n===== Starting agent runs at {datetime.now().isoformat()} =====\")\n",
        "    for agent in agents:\n",
        "        try:\n",
        "            agent.run()\n",
        "        except Exception as e:\n",
        "            print(f\"Error running agent for {agent.company_name}: {e}\")\n",
        "    print(f\"===== All agent runs completed at {datetime.now().isoformat()} =====\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Initializes an agent based on user input and runs it.\"\"\"\n",
        "    company_name = input(\"Enter company name: \")\n",
        "    description = input(\"Enter company description (press Enter for default): \")\n",
        "    if not description:\n",
        "        description = f\"{company_name} is a well-known company.\"\n",
        "    twitter_handle = company_name.replace(\" \", \"\")\n",
        "\n",
        "    agent = CompanyAgent(\n",
        "        company_name,\n",
        "        description,\n",
        "        twitter_handle,\n",
        "        twitter_api_key=os.environ.get(\"TWITTER_API_KEY\"),\n",
        "        twitter_api_secret=os.environ.get(\"TWITTER_API_SECRET\"),\n",
        "        twitter_access_token=os.environ.get(\"TWITTER_ACCESS_TOKEN\"),\n",
        "        twitter_access_token_secret=os.environ.get(\"TWITTER_ACCESS_TOKEN_SECRET\"),\n",
        "        mediastack_api_key=os.environ.get(\"MEDIASTACK_API_KEY\"),\n",
        "        serpapi_api_key=os.environ.get(\"SERPAPI_API_KEY\")\n",
        "    )\n",
        "\n",
        "    # Run one cycle then exit\n",
        "    trigger_training([agent])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}